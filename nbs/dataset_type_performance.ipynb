{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License.\n",
    "\n",
    "# PersistentDataset, CacheDataset, LMDBDataset, and simple Dataset Tutorial and Speed Test\n",
    "\n",
    "This tutorial shows how to accelerate PyTorch medical DL program based on\n",
    "how data is loaded and preprocessed using different MONAI `Dataset` managers.\n",
    "\n",
    "`Dataset` provides the simplest model of data loading.  Each time a dataset is needed, it is reloaded from the original datasources, and processed through the all non-random and random transforms to generate analyzable tensors. This mechanism has the smallest memory footprint, and the smallest temporary disk footprint.\n",
    "\n",
    "`CacheDataset` provides a mechanism to pre-load all original data and apply non-random transforms into analyzable tensors loaded in memory prior to starting analysis.  The `CacheDataset` requires all tensor representations of data requested to be loaded into memory at once. The subset of random transforms is applied to the cached components before use. This is the highest performance dataset if all data fit in core memory.\n",
    "\n",
    "`PersistentDataset` processes original data sources through the non-random transforms on first use, and stores these intermediate tensor values to an on-disk persistence representation.  The intermediate processed tensors are loaded from disk on each use for processing by the random-transforms for each analysis request.  The `PersistentDataset` has a similar memory footprint to the simple `Dataset`, with performance characteristics close to the `CacheDataset` at the expense of disk storage.  Additionally, the cost of first time processing of data is distributed across each first use.\n",
    "\n",
    "`LMDBDataset` is a variant of `PersistentDataset`. It uses an LMDB database as the persistent backend.\n",
    "\n",
    "It's modified from the [Spleen 3D segmentation tutorial notebook](../3d_segmentation/spleen_segmentation_3d.ipynb).\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/acceleration/dataset_type_performance.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm, lmdb]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.5.0+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: /home/<username>/project/coronary-artery/.venv/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.3.2\n",
      "scikit-image version: 0.25.2\n",
      "scipy version: 1.15.2\n",
      "Pillow version: 11.1.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.20.0+cu121\n",
      "tqdm version: 4.67.1\n",
      "lmdb version: 1.6.2\n",
      "psutil version: 7.0.0\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.1\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import autorootcwd\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import (\n",
    "    CacheDataset,\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    "    LMDBDataset,\n",
    "    PersistentDataset,\n",
    "    decollate_batch,\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from src.losses.losses import DiceFocalLoss\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric, MeanIoU\n",
    "from monai.networks.layers import Norm\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    EnsureType,\n",
    "    Resize,\n",
    "    RandShiftIntensityd,\n",
    "    RandFlipd,\n",
    "    GaussianSmoothd\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from dvclive.lightning import DVCLiveLogger\n",
    "import nibabel as nib\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
    "This allows you to save results and reuse downloads.  \n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/seoooa/project/coronary-artery/data/imageCAS_test\"\n",
    "cache_root = \"/data/seoooa/cache/imageCAS_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 100\n",
      "Validation samples: 10\n",
      "Test samples: 5\n"
     ]
    }
   ],
   "source": [
    "def load_data_from_folder(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for subdir in sorted(glob.glob(os.path.join(folder_path, \"*\"))):\n",
    "        if os.path.isdir(subdir):\n",
    "            img_file = glob.glob(os.path.join(subdir, \"img.nii.gz\"))\n",
    "            label_file = glob.glob(os.path.join(subdir, \"label.nii.gz\"))\n",
    "            if img_file and label_file:\n",
    "                images.extend(img_file)\n",
    "                labels.extend(label_file)\n",
    "    return [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(images, labels)]\n",
    "\n",
    "train_files = load_data_from_folder(os.path.join(data_root, \"train\"))\n",
    "val_files = load_data_from_folder(os.path.join(data_root, \"valid\"))\n",
    "test_files = load_data_from_folder(os.path.join(data_root, \"test\"))\n",
    "\n",
    "print(f\"Training samples: {len(train_files)}\")\n",
    "print(f\"Validation samples: {len(val_files)}\")\n",
    "print(f\"Test samples: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoronaryArteryDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_ds, val_ds, test_ds, batch_size=1, num_workers=8):\n",
    "        super().__init__()\n",
    "        self.train_ds = train_ds\n",
    "        self.val_ds = val_ds\n",
    "        self.test_ds = test_ds\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=2\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=2\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.models.proposed.segresnet import SegResNet\n",
    "\n",
    "class CoronaryArterySegmentModel(pl.LightningModule):\n",
    "    def __init__(self, batch_size=1, lr=1e-3, patch_size=(96, 96, 96), log_dir=\"nbs/result/dataset_performance/Dataset\"):\n",
    "        super().__init__()\n",
    "        self._model = SegResNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=2,\n",
    "            init_filters=16,\n",
    "            blocks_down=(1, 2, 2, 4),\n",
    "            blocks_up=(1, 1, 1),\n",
    "            dropout_prob=0.2,\n",
    "            label_nc=8,\n",
    "        )\n",
    "        \n",
    "        self.loss_function = DiceFocalLoss(\n",
    "            to_onehot_y=True,\n",
    "            softmax=True,\n",
    "        )\n",
    "\n",
    "        self.post_pred = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(argmax=True, to_onehot=2)])\n",
    "        self.post_label = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(to_onehot=2)])\n",
    "\n",
    "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "        self.hausdorff_metric = HausdorffDistanceMetric(\n",
    "            include_background=False, \n",
    "            percentile=85,\n",
    "            directed=False,\n",
    "            reduction=\"mean\"\n",
    "        )\n",
    "        self.mean_iou_metric = MeanIoU(include_background=False, reduction=\"mean\")\n",
    "\n",
    "        self.best_val_dice = 0\n",
    "        self.best_val_epoch = 0\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.patch_size = patch_size\n",
    "        self.result_folder = Path(log_dir)\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.val_dice_history = []\n",
    "        self.epoch_times = []\n",
    "        self.epoch_start_time = None\n",
    "\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        return self._model(x, seg)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self._model.parameters(), self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if self.epoch_start_time is not None:\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            self.epoch_times.append(epoch_time)\n",
    "            self.log(\"epoch_time\", epoch_time)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels, segs = (\n",
    "            batch[\"image\"],\n",
    "            batch[\"label\"],\n",
    "            batch[\"seg\"],\n",
    "        )\n",
    "        output = self.forward(images, segs)\n",
    "        loss = self.loss_function(output, labels)\n",
    "        metrics = loss.item()\n",
    "\n",
    "        # 학습 손실 저장\n",
    "        self.train_loss_history.append(metrics)\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            metrics,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels, segs = (\n",
    "            batch[\"image\"],\n",
    "            batch[\"label\"],\n",
    "            batch[\"seg\"],\n",
    "        )\n",
    "\n",
    "        inputs = torch.cat((images, segs), dim=1)\n",
    "        roi_size = self.patch_size\n",
    "        sw_batch_size = 4\n",
    "        \n",
    "        outputs = sliding_window_inference(\n",
    "            inputs,\n",
    "            roi_size,\n",
    "            sw_batch_size,\n",
    "            lambda x: self.forward(x[:, :1, ...], x[:, 1:, ...])\n",
    "        )\n",
    "\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
    "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "        self.mean_iou_metric(y_pred=outputs, y=labels)\n",
    "\n",
    "        # Hausdorff\n",
    "        try:\n",
    "            resize_transform = Resize(\n",
    "                spatial_size=[60, 60, 60],\n",
    "                mode=\"nearest\"\n",
    "            )\n",
    "            \n",
    "            downsampled_outputs = [resize_transform(i) for i in outputs]\n",
    "            downsampled_labels = [resize_transform(i) for i in labels]\n",
    "            \n",
    "            for idx in range(len(downsampled_outputs)):\n",
    "                self.hausdorff_metric(\n",
    "                    y_pred=downsampled_outputs[idx].unsqueeze(0),\n",
    "                    y=downsampled_labels[idx].unsqueeze(0)\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Hausdorff metric calculation error: {e}\")\n",
    "\n",
    "        d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
    "        self.validation_step_outputs.append(d)\n",
    "        return d\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_loss, num_items = 0, 0\n",
    "        for output in self.validation_step_outputs:\n",
    "            val_loss += output[\"val_loss\"].sum().item()\n",
    "            num_items += output[\"val_number\"]\n",
    "\n",
    "        mean_val_dice = self.dice_metric.aggregate().item()\n",
    "        mean_val_hausdorff = self.hausdorff_metric.aggregate().item()\n",
    "        mean_val_iou = self.mean_iou_metric.aggregate().item()\n",
    "\n",
    "        # Validation Dice 점수 저장\n",
    "        self.val_dice_history.append(mean_val_dice)\n",
    "\n",
    "        self.dice_metric.reset()\n",
    "        self.hausdorff_metric.reset()\n",
    "        self.mean_iou_metric.reset()\n",
    "\n",
    "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
    "        log_dict = {\n",
    "            \"val_dice\": mean_val_dice,\n",
    "            \"val_hausdorff\": mean_val_hausdorff,\n",
    "            \"val_iou\": mean_val_iou,\n",
    "            \"val_loss\": mean_val_loss,\n",
    "        }\n",
    "\n",
    "        self.log_dict(log_dict)\n",
    "\n",
    "        if mean_val_dice > self.best_val_dice:\n",
    "            self.best_val_dice = mean_val_dice\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "        print(\n",
    "            f\"current epoch: {self.current_epoch} \"\n",
    "            f\"current mean dice: {mean_val_dice:.4f}, \"\n",
    "            f\"hausdorff: {mean_val_hausdorff:.4f}, \"\n",
    "            f\"iou: {mean_val_iou:.4f}\"\n",
    "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
    "            f\"at epoch: {self.best_val_epoch}\"\n",
    "        )\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def save_result(self, inputs, outputs, labels, filename_prefix=\"result\"):\n",
    "        save_folder = self.result_folder / \"test\"\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "        inputs_np = inputs.detach().cpu().numpy().squeeze()\n",
    "        outputs_np = outputs.detach().cpu().numpy().squeeze()[1]\n",
    "        labels_np = labels.detach().cpu().numpy().squeeze()[1]\n",
    "\n",
    "        # Save inputs as NIfTI\n",
    "        inputs_nifti = nib.Nifti1Image(\n",
    "            inputs_np,\n",
    "            np.array([[0.35, 0, 0, 0], [0, 0.35, 0, 0], [0, 0, 0.5, 0], [0, 0, 0, 1]]),\n",
    "        )\n",
    "        nib.save(inputs_nifti, save_folder / f\"{filename_prefix}_inputs.nii.gz\")\n",
    "\n",
    "        # Save outputs as NIfTI\n",
    "        outputs_nifti = nib.Nifti1Image(\n",
    "            outputs_np,\n",
    "            np.array([[0.35, 0, 0, 0], [0, 0.35, 0, 0], [0, 0, 0.5, 0], [0, 0, 0, 1]]),\n",
    "        )\n",
    "        nib.save(outputs_nifti, save_folder / f\"{filename_prefix}_outputs.nii.gz\")\n",
    "\n",
    "        # Save labels as NIfTI\n",
    "        labels_nifti = nib.Nifti1Image(\n",
    "            labels_np,\n",
    "            np.array([[0.35, 0, 0, 0], [0, 0.35, 0, 0], [0, 0, 0.5, 0], [0, 0, 0, 1]]),\n",
    "        )\n",
    "        nib.save(labels_nifti, save_folder / f\"{filename_prefix}_labels.nii.gz\")\n",
    "\n",
    "        print(f\"Result saved to: {save_folder}\")\n",
    "        print(f\"Inputs: {save_folder / f'{filename_prefix}_inputs.nii.gz'}\")\n",
    "        print(f\"Outputs: {save_folder / f'{filename_prefix}_outputs.nii.gz'}\")\n",
    "        print(f\"Labels: {save_folder / f'{filename_prefix}_labels.nii.gz'}\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels, segs = (\n",
    "            batch[\"image\"],\n",
    "            batch[\"label\"],\n",
    "            batch[\"seg\"],\n",
    "        )\n",
    "        roi_size = self.patch_size\n",
    "        sw_batch_size = 4\n",
    "        inputs = torch.cat((images, segs), dim=1)\n",
    "\n",
    "        outputs = sliding_window_inference(\n",
    "            inputs,\n",
    "            roi_size,\n",
    "            sw_batch_size,\n",
    "            lambda x: self.forward(x[:, :1, ...], x[:, 1:, ...])\n",
    "        )\n",
    "\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
    "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "\n",
    "        filename = batch[\"image\"].meta[\"filename_or_obj\"][0]\n",
    "        patient_id = filename.split(\"/\")[-2]  # Gets patient id from the path\n",
    "\n",
    "        # Save result\n",
    "        self.save_result(\n",
    "            images, outputs[0], labels[0], filename_prefix=f\"Subj_{patient_id}\"\n",
    "        )\n",
    "\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "        self.mean_iou_metric(y_pred=outputs, y=labels)\n",
    "\n",
    "        try:\n",
    "            resize_transform = Resize(\n",
    "                spatial_size=[60, 60, 60],\n",
    "                mode=\"nearest\"\n",
    "            )\n",
    "            \n",
    "            downsampled_outputs = [resize_transform(i) for i in outputs]\n",
    "            downsampled_labels = [resize_transform(i) for i in labels]\n",
    "            \n",
    "            for idx in range(len(downsampled_outputs)):\n",
    "                self.hausdorff_metric(\n",
    "                    y_pred=downsampled_outputs[idx].unsqueeze(0),\n",
    "                    y=downsampled_labels[idx].unsqueeze(0)\n",
    "                )\n",
    "            \n",
    "            hausdorff_score = self.hausdorff_metric.aggregate().item()\n",
    "            self.hausdorff_metric.reset()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Hausdorff metric calculation error in test step: {e}\")\n",
    "            hausdorff_score = 0\n",
    "        \n",
    "        dice_score = self.dice_metric.aggregate().item()\n",
    "        # hausdorff_score = self.hausdorff_metric.aggregate().item()\n",
    "        mean_iou_score = self.mean_iou_metric.aggregate().item()\n",
    "        \n",
    "        d = {\n",
    "            \"test_dice\": dice_score,\n",
    "            \"test_hausdorff\": hausdorff_score,\n",
    "            \"test_iou\": mean_iou_score,\n",
    "            \"patient_id\": patient_id,\n",
    "        }\n",
    "        self.test_step_outputs.append(d)\n",
    "\n",
    "        self.dice_metric.reset()\n",
    "        self.mean_iou_metric.reset()\n",
    "\n",
    "        return d\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Calculate mean metrics\n",
    "        dice_scores = [x[\"test_dice\"] for x in self.test_step_outputs]\n",
    "        hausdorff_scores = [x[\"test_hausdorff\"] for x in self.test_step_outputs]\n",
    "        iou_scores = [x[\"test_iou\"] for x in self.test_step_outputs]\n",
    "\n",
    "        mean_dice = np.mean(dice_scores)\n",
    "        mean_hausdorff = np.mean(hausdorff_scores)\n",
    "        mean_iou = np.mean(iou_scores)\n",
    "\n",
    "        # Log mean metrics\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"test/mean_dice\": mean_dice,\n",
    "                \"test/mean_hausdorff\": mean_hausdorff,\n",
    "                \"test/mean_iou\": mean_iou,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Save detailed result to CSV\n",
    "        result_file = self.result_folder / \"test\" / \"test_result.csv\"\n",
    "        with open(result_file, \"w\", newline=\"\") as csvfile:\n",
    "            fieldnames = [\"dice_score\", \"hausdorff_score\", \"iou_score\", \"patient_id\"]\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "            for result in self.test_step_outputs:\n",
    "                result_with_filename = {\n",
    "                    \"dice_score\": result[\"test_dice\"],\n",
    "                    \"hausdorff_score\": result[\"test_hausdorff\"],\n",
    "                    \"iou_score\": result[\"test_iou\"],\n",
    "                    \"patient_id\": result[\"patient_id\"],\n",
    "                }\n",
    "                writer.writerow(result_with_filename)\n",
    "\n",
    "        # Write summary row\n",
    "        with open(result_file, \"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"dice_score\": f\"{mean_dice:.4f} ± {np.std(dice_scores):.4f}\",\n",
    "                    \"hausdorff_score\": f\"{mean_hausdorff:.4f} ± {np.std(hausdorff_scores):.4f}\",\n",
    "                    \"iou_score\": f\"{mean_iou:.4f} ± {np.std(iou_scores):.4f}\",\n",
    "                    \"patient_id\": f\"AVG ± STD\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print(f\"\\nTest Result Summary:\")\n",
    "        print(f\"Mean Dice Score: {mean_dice:.4f}\")\n",
    "        print(f\"Mean Hausdorff Distance: {mean_hausdorff:.4f}\")\n",
    "        print(f\"Mean IoU Score: {mean_iou:.4f}\")\n",
    "        print(f\"Detailed result saved to: {result_file}\")\n",
    "\n",
    "        self.test_step_outputs.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import (\n",
    "    BatchSizeFinder,\n",
    "    LearningRateFinder,\n",
    "    StochasticWeightAveraging,\n",
    ")\n",
    "\n",
    "def train_process(train_ds, val_ds, test_ds, log_dir):\n",
    "    # Set up data module\n",
    "    data_module = CoronaryArteryDataModule(\n",
    "        train_ds=train_ds,\n",
    "        val_ds=val_ds,\n",
    "        test_ds=test_ds,\n",
    "        batch_size=1,\n",
    "        num_workers=8\n",
    "    )\n",
    "\n",
    "    # Set up model\n",
    "    model = CoronaryArterySegmentModel(\n",
    "        batch_size=1,\n",
    "        lr=1e-3,\n",
    "        patch_size=(96, 96, 96),\n",
    "        log_dir=log_dir\n",
    "    )\n",
    "\n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        StochasticWeightAveraging(\n",
    "            swa_lrs=[1e-4],\n",
    "            annealing_epochs=5,\n",
    "            swa_epoch_start=100\n",
    "        )\n",
    "    ]\n",
    "    # dvc_logger = DVCLiveLogger(log_model=True, dir=log_dir, report=\"html\")\n",
    "\n",
    "    # Set up trainer\n",
    "    trainer = pl.Trainer(\n",
    "        devices=[0],  # GPU 설정\n",
    "        # strategy=\"ddp_notebook\",\n",
    "        max_epochs=20,\n",
    "        enable_checkpointing=True,\n",
    "        benchmark=True,\n",
    "        accumulate_grad_batches=5,\n",
    "        precision=\"bf16-mixed\",\n",
    "        check_val_every_n_epoch=5,\n",
    "        num_sanity_val_steps=1,\n",
    "        callbacks=callbacks,\n",
    "        default_root_dir=log_dir,\n",
    "        # enable_progress_bar=False\n",
    "    )\n",
    "\n",
    "    # Train and test\n",
    "    print(\"\\nStarting training...\")\n",
    "    total_start = time.time()\n",
    "    \n",
    "    trainer.fit(model, data_module)\n",
    "    train_end = time.time()\n",
    "    train_time = train_end - total_start\n",
    "    print(f\"\\nTraining completed in {train_time:.2f} seconds ({train_time/3600:.2f} hours)\")\n",
    "    \n",
    "    trainer.save_checkpoint(os.path.join(log_dir, \"final_model.ckpt\"))\n",
    "    print(\"\\nStarting testing...\")\n",
    "    \n",
    "    trainer.test(model, data_module)\n",
    "    test_end = time.time()\n",
    "    test_time = test_end - train_end\n",
    "    total_time = test_end - total_start\n",
    "    \n",
    "    print(f\"\\nTime Summary:\")\n",
    "    print(f\"Training Time: {train_time:.2f} seconds ({train_time/3600:.2f} hours)\")\n",
    "    print(f\"Testing Time: {test_time:.2f} seconds ({test_time/3600:.2f} hours)\")\n",
    "    print(f\"Total Time: {total_time:.2f} seconds ({total_time/3600:.2f} hours)\")\n",
    "\n",
    "    return (\n",
    "        trainer.max_epochs,\n",
    "        total_time,\n",
    "        model.train_loss_history,  # 직접 저장한 메트릭 사용\n",
    "        model.val_dice_history,    # 직접 저장한 메트릭 사용\n",
    "        model.epoch_times          # 직접 저장한 메트릭 사용\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of speed testing\n",
    "\n",
    "The `PersistenceDataset`, `CacheDataset`, and `Dataset` are compared for speed for running 30 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set ImageCAS dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/seoooa/project/coronary-artery/data/imageCAS_test\"\n",
    "cache_root = \"/data/seoooa/cache/imageCAS_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_folder(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    segs = [] \n",
    "    \n",
    "    for subdir in sorted(glob.glob(os.path.join(folder_path, \"*\"))):\n",
    "        if os.path.isdir(subdir):\n",
    "            img_file = glob.glob(os.path.join(subdir, \"img.nii.gz\"))\n",
    "            label_file = glob.glob(os.path.join(subdir, \"label.nii.gz\"))\n",
    "            seg_file = glob.glob(os.path.join(subdir, \"heart_combined.nii.gz\"))\n",
    "            \n",
    "            if img_file and label_file and seg_file:\n",
    "                images.extend(img_file)\n",
    "                labels.extend(label_file)\n",
    "                segs.extend(seg_file)\n",
    "                \n",
    "    return [{\n",
    "        \"image\": image_name,\n",
    "        \"label\": label_name,\n",
    "        \"seg\": seg_name\n",
    "    } for image_name, label_name, seg_name in zip(images, labels, segs)]\n",
    "\n",
    "train_files = load_data_from_folder(os.path.join(data_root, \"train\"))\n",
    "val_files = load_data_from_folder(os.path.join(data_root, \"valid\"))\n",
    "test_files = load_data_from_folder(os.path.join(data_root, \"test\"))\n",
    "\n",
    "print(f\"Training samples: {len(train_files)}\")\n",
    "print(f\"Validation samples: {len(val_files)}\")\n",
    "print(f\"Test samples: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transforms for training and validation\n",
    "\n",
    "Deterministic transforms during training:\n",
    "* LoadImaged\n",
    "* EnsureChannelFirstd\n",
    "* Spacingd\n",
    "* Orientationd\n",
    "* ScaleIntensityRanged\n",
    "\n",
    "Non-deterministic transforms:\n",
    "* RandCropByPosNegLabeld\n",
    "\n",
    "All the validation transforms are deterministic.\n",
    "The results of all the deterministic transforms will be cached to accelerate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformations():\n",
    "    train_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\", \"label\", \"seg\"]),\n",
    "            EnsureChannelFirstd(keys=[\"image\", \"label\", \"seg\"]),\n",
    "            Orientationd(keys=[\"image\", \"label\", \"seg\"], axcodes=\"RAS\"),\n",
    "            # Spacingd(\n",
    "            #    keys=[\"image\", \"label\"],\n",
    "            #    pixdim=(0.35, 0.35, 0.5),\n",
    "            #    mode=(\"bilinear\", \"nearest\"),\n",
    "            # ),\n",
    "            ScaleIntensityRanged(\n",
    "                keys=[\"image\"],\n",
    "                a_min=-150,\n",
    "                a_max=550,\n",
    "                b_min=0.0,\n",
    "                b_max=1.0,\n",
    "                clip=True,\n",
    "            ),\n",
    "            AsDiscreted(\n",
    "                keys=[\"seg\"],\n",
    "                to_onehot=8,\n",
    "            ),\n",
    "            CropForegroundd(keys=[\"image\", \"label\", \"seg\"], source_key=\"image\"),\n",
    "            RandCropByPosNegLabeld(\n",
    "                keys=[\"image\", \"label\", \"seg\"],\n",
    "                label_key=\"label\",\n",
    "                spatial_size=(96, 96, 96),\n",
    "                pos=1,\n",
    "                neg=1,\n",
    "                num_samples=4,\n",
    "                image_key=\"image\",\n",
    "                image_threshold=0,\n",
    "            ),\n",
    "            RandFlipd(\n",
    "                keys=[\"image\", \"label\", \"seg\"],\n",
    "                spatial_axis=[0],\n",
    "                prob=0.10,\n",
    "            ),\n",
    "            RandFlipd(\n",
    "                keys=[\"image\", \"label\", \"seg\"],\n",
    "                spatial_axis=[1],\n",
    "                prob=0.10,\n",
    "            ),\n",
    "            GaussianSmoothd(keys=[\"seg\"], sigma=1.0),\n",
    "            RandShiftIntensityd(keys=\"image\", offsets=0.05, prob=0.5),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # NOTE: No random cropping in the validation data,\n",
    "    # we will evaluate the entire image using a sliding window.\n",
    "    val_transforms = Compose(\n",
    "        [\n",
    "            # LoadImaged with image_only=True is to return the MetaTensors\n",
    "            # the additional metadata dictionary is not returned.\n",
    "            LoadImaged(keys=[\"image\", \"label\", \"seg\"]),\n",
    "            EnsureChannelFirstd(keys=[\"image\", \"label\", \"seg\"]),\n",
    "            Orientationd(keys=[\"image\", \"label\", \"seg\"], axcodes=\"RAS\"),\n",
    "            # Spacingd(\n",
    "            #    keys=[\"image\", \"label\"],\n",
    "            #    pixdim=(0.35, 0.35, 0.5),\n",
    "            #    mode=(\"bilinear\", \"nearest\"),\n",
    "            # ),\n",
    "            ScaleIntensityRanged(\n",
    "                keys=[\"image\"],\n",
    "                a_min=-150,\n",
    "                a_max=550,\n",
    "                b_min=0.0,\n",
    "                b_max=1.0,\n",
    "                clip=True,\n",
    "            ),\n",
    "            AsDiscreted(\n",
    "                keys=[\"seg\"],\n",
    "                to_onehot=8,\n",
    "            ),\n",
    "            CropForegroundd(keys=[\"image\", \"label\", \"seg\"], source_key=\"image\"),\n",
    "            GaussianSmoothd(keys=[\"seg\"], sigma=1.0),\n",
    "        ]\n",
    "    )\n",
    "    return train_transforms, val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_results_to_csv(filename, epoch_loss_values, metric_values, epoch_times, total_time, max_epochs, init_time=None, disk_usage_before=None, disk_usage_after=None):\n",
    "    with open(filename, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"max_epochs\", max_epochs])\n",
    "        writer.writerow([\"total_time\", total_time])\n",
    "        if init_time is not None:\n",
    "            writer.writerow([\"init_time\", init_time])\n",
    "        \n",
    "        if disk_usage_before is not None:\n",
    "            writer.writerow([\"disk_usage_before_total_GB\", disk_usage_before[0] // (2**30)])\n",
    "            writer.writerow([\"disk_usage_before_used_GB\", disk_usage_before[1] // (2**30)])\n",
    "            writer.writerow([\"disk_usage_before_free_GB\", disk_usage_before[2] // (2**30)])\n",
    "        \n",
    "        if disk_usage_after is not None:\n",
    "            writer.writerow([\"disk_usage_after_total_GB\", disk_usage_after[0] // (2**30)])\n",
    "            writer.writerow([\"disk_usage_after_used_GB\", disk_usage_after[1] // (2**30)])\n",
    "            writer.writerow([\"disk_usage_after_free_GB\", disk_usage_after[2] // (2**30)])\n",
    "            \n",
    "        writer.writerow([\"epoch\", \"loss\", \"metric\", \"epoch_time\"])\n",
    "        for i in range(len(epoch_loss_values)):\n",
    "            loss = epoch_loss_values[i] if i < len(epoch_loss_values) else \"\"\n",
    "            metric = metric_values[i] if i < len(metric_values) else \"\"\n",
    "            epoch_time = epoch_times[i] if i < len(epoch_times) else \"\"\n",
    "            writer.writerow([i+1, loss, metric, epoch_time])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable deterministic training and regular `Dataset`\n",
    "\n",
    "Load each original dataset and transform each time it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n",
    "log_dir = f\"nbs/result/dataset_performance/Dataset\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "train_trans, val_trans = transformations()\n",
    "train_ds = Dataset(data=train_files, transform=train_trans)\n",
    "val_ds = Dataset(data=val_files, transform=val_trans)\n",
    "test_ds = Dataset(data=test_files, transform=val_trans)\n",
    "\n",
    "(\n",
    "    max_epochs,\n",
    "    total_time,\n",
    "    epoch_loss_values,\n",
    "    metric_values,\n",
    "    epoch_times,\n",
    ") = train_process(train_ds, val_ds, test_ds, log_dir)\n",
    "\n",
    "print(f\"total training time of {max_epochs} epochs\" f\" with regular Dataset: {total_time:.4f}\")\n",
    "\n",
    "save_results_to_csv(\n",
    "    \"nbs/result/dataset_performance/regular_results.csv\",\n",
    "    epoch_loss_values, metric_values, epoch_times, total_time, max_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable deterministic training and `PersistentDataset`\n",
    "\n",
    "Use persistent storage of non-random transformed training and validation data computed once and stored in persistently across runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For SSD Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n",
    "log_dir = f\"nbs/result/dataset_performance/PersistentDataset\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "persistent_cache = Path(\"nbs/result/dataset_performance/PersistentDataset/persistent_cache\")\n",
    "os.makedirs(persistent_cache, exist_ok=True)\n",
    "\n",
    "import shutil\n",
    "disk_usage_before = shutil.disk_usage(persistent_cache)\n",
    "print(f\"----------Disk Status before persistent cache:\")\n",
    "print(f\"Total: {disk_usage_before[0] // (2**30)} GB\")\n",
    "print(f\"Used: {disk_usage_before[1] // (2**30)} GB\")\n",
    "print(f\"Free: {disk_usage_before[2] // (2**30)} GB\")\n",
    "\n",
    "train_cache = persistent_cache / \"train\"\n",
    "valid_cache = persistent_cache / \"valid\"\n",
    "test_cache = persistent_cache / \"test\"\n",
    "\n",
    "os.makedirs(train_cache, exist_ok=True)\n",
    "os.makedirs(valid_cache, exist_ok=True)\n",
    "os.makedirs(test_cache, exist_ok=True)\n",
    "\n",
    "train_trans, val_trans = transformations()\n",
    "persistent_init_start = time.time()\n",
    "\n",
    "train_persitence_ds = PersistentDataset(data=train_files, transform=train_trans, cache_dir=str(train_cache))\n",
    "val_persitence_ds = PersistentDataset(data=val_files, transform=val_trans, cache_dir=str(valid_cache))\n",
    "test_persitence_ds = PersistentDataset(data=test_files, transform=val_trans, cache_dir=str(test_cache))\n",
    "\n",
    "persistence_init_time = time.time() - persistent_init_start\n",
    "\n",
    "(\n",
    "    persistence_epoch_num,\n",
    "    persistence_total_time,\n",
    "    persistence_epoch_loss_values,\n",
    "    persistence_metric_values,\n",
    "    persistence_epoch_times,\n",
    ") = train_process(train_persitence_ds, val_persitence_ds, test_persitence_ds, log_dir)\n",
    "print(\n",
    "    f\"total training time of {persistence_epoch_num}\"\n",
    "    f\" epochs with persistent storage Dataset: {persistence_total_time:.4f}\"\n",
    ")\n",
    "\n",
    "disk_usage_after = shutil.disk_usage(persistent_cache)\n",
    "print(f\"----------Disk Status after persistent cache:\")\n",
    "print(f\"Total: {disk_usage_after[0] // (2**30)} GB\")\n",
    "print(f\"Used: {disk_usage_after[1] // (2**30)} GB\")\n",
    "print(f\"Free: {disk_usage_after[2] // (2**30)} GB\")\n",
    "\n",
    "save_results_to_csv(\n",
    "    \"nbs/result/dataset_performance/persistent_results.csv\",\n",
    "    persistence_epoch_loss_values, \n",
    "    persistence_metric_values, \n",
    "    persistence_epoch_times, \n",
    "    persistence_total_time, \n",
    "    persistence_epoch_num,\n",
    "    init_time=persistence_init_time,\n",
    "    disk_usage_before=disk_usage_before,\n",
    "    disk_usage_after=disk_usage_after\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For HDD Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n",
    "log_dir = f\"/data/seoooa/cache/dataset_performance/PersistentDataset\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "persistent_cache = Path(\"/data/seoooa/cache/dataset_performance/PersistentDataset/persistent_cache\")\n",
    "os.makedirs(persistent_cache, exist_ok=True)\n",
    "\n",
    "import shutil\n",
    "disk_usage_before = shutil.disk_usage(persistent_cache)\n",
    "print(f\"----------Disk Status before persistent cache:\")\n",
    "print(f\"Total: {disk_usage_before[0] // (2**30)} GB\")\n",
    "print(f\"Used: {disk_usage_before[1] // (2**30)} GB\")\n",
    "print(f\"Free: {disk_usage_before[2] // (2**30)} GB\")\n",
    "\n",
    "train_cache = persistent_cache / \"train\"\n",
    "valid_cache = persistent_cache / \"valid\"\n",
    "test_cache = persistent_cache / \"test\"\n",
    "\n",
    "os.makedirs(train_cache, exist_ok=True)\n",
    "os.makedirs(valid_cache, exist_ok=True)\n",
    "os.makedirs(test_cache, exist_ok=True)\n",
    "\n",
    "train_trans, val_trans = transformations()\n",
    "persistent_init_start = time.time()\n",
    "\n",
    "train_persitence_ds = PersistentDataset(data=train_files, transform=train_trans, cache_dir=str(train_cache))\n",
    "val_persitence_ds = PersistentDataset(data=val_files, transform=val_trans, cache_dir=str(valid_cache))\n",
    "test_persitence_ds = PersistentDataset(data=test_files, transform=val_trans, cache_dir=str(test_cache))\n",
    "\n",
    "persistence_init_time = time.time() - persistent_init_start\n",
    "\n",
    "(\n",
    "    persistence_epoch_num,\n",
    "    persistence_total_time,\n",
    "    persistence_epoch_loss_values,\n",
    "    persistence_metric_values,\n",
    "    persistence_epoch_times,\n",
    ") = train_process(train_persitence_ds, val_persitence_ds, test_persitence_ds, log_dir)\n",
    "print(\n",
    "    f\"total training time of {persistence_epoch_num}\"\n",
    "    f\" epochs with persistent storage Dataset: {persistence_total_time:.4f}\"\n",
    ")\n",
    "\n",
    "disk_usage_after = shutil.disk_usage(persistent_cache)\n",
    "print(f\"----------Disk Status after persistent cache:\")\n",
    "print(f\"Total: {disk_usage_after[0] // (2**30)} GB\")\n",
    "print(f\"Used: {disk_usage_after[1] // (2**30)} GB\")\n",
    "print(f\"Free: {disk_usage_after[2] // (2**30)} GB\")\n",
    "\n",
    "save_results_to_csv(\n",
    "    \"nbs/result/dataset_performance/persistent_results.csv\",\n",
    "    persistence_epoch_loss_values, \n",
    "    persistence_metric_values, \n",
    "    persistence_epoch_times, \n",
    "    persistence_total_time, \n",
    "    persistence_epoch_num,\n",
    "    init_time=persistence_init_time,\n",
    "    disk_usage_before=disk_usage_before,\n",
    "    disk_usage_after=disk_usage_after\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable deterministic training and `LMDBDataset`\n",
    "\n",
    "Use persistent storage of non-random transformed training and validation data computed once and stored in persistently using LMDB across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def check_lmdb_cache(cache_dir: str) -> bool:\n",
    "    lmdb_file = os.path.join(cache_dir, \"monai_cache.lmdb\")\n",
    "    return os.path.exists(lmdb_file)\n",
    "\n",
    "# LMDB 캐시 디렉토리 설정\n",
    "LMDB_cache = Path(\"nbs/result/dataset_performance/LMDBDataset/lmdb_cache\")\n",
    "os.makedirs(LMDB_cache, exist_ok=True)\n",
    "\n",
    "check_lmdb_cache(LMDB_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "set_determinism(seed=0)\n",
    "log_dir = f\"nbs/result/dataset_performance/LMDBDataset\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "LMDB_cache = Path(\"nbs/result/dataset_performance/LMDBDataset/lmdb_cache\")\n",
    "os.makedirs(LMDB_cache, exist_ok=True)\n",
    "\n",
    "import shutil\n",
    "disk_usage_before = shutil.disk_usage(LMDB_cache)\n",
    "print(f\"----------Disk Status before LMDB cache:\")\n",
    "print(f\"Total: {disk_usage_before[0] // (2**30)} GB\")\n",
    "print(f\"Used: {disk_usage_before[1] // (2**30)} GB\")\n",
    "print(f\"Free: {disk_usage_before[2] // (2**30)} GB\")\n",
    "\n",
    "train_trans, val_trans = transformations()\n",
    "lmdb_init_start = time.time()\n",
    "\n",
    "lmdb_kwargs = {\n",
    "    \"map_async\": True,\n",
    "    \"map_size\": 1024 * 1024 * 1024 * 10,  # 10GB\n",
    "    \"writemap\": True,\n",
    "    \"readonly\": check_lmdb_cache(LMDB_cache)  # 캐시가 있으면 읽기 전용으로 설정\n",
    "}\n",
    "\n",
    "train_lmdb_ds = LMDBDataset(\n",
    "    data=train_files,\n",
    "    transform=train_trans,\n",
    "    cache_dir=LMDB_cache,\n",
    "    lmdb_kwargs=lmdb_kwargs\n",
    ")\n",
    "val_lmdb_ds = LMDBDataset(\n",
    "    data=val_files,\n",
    "    transform=val_trans,\n",
    "    cache_dir=LMDB_cache,\n",
    "    lmdb_kwargs=lmdb_kwargs\n",
    ")\n",
    "test_lmdb_ds = LMDBDataset(\n",
    "    data=test_files,\n",
    "    transform=val_trans,\n",
    "    cache_dir=LMDB_cache,\n",
    "    lmdb_kwargs=lmdb_kwargs\n",
    ")\n",
    "\n",
    "lmdb_init_time = time.time() - lmdb_init_start\n",
    "\n",
    "(\n",
    "    lmdb_epoch_num,\n",
    "    lmdb_total_time,\n",
    "    lmdb_epoch_loss_values,\n",
    "    lmdb_metric_values,\n",
    "    lmdb_epoch_times,\n",
    ") = train_process(train_lmdb_ds, val_lmdb_ds, test_lmdb_ds, log_dir)\n",
    "print(f\"total training time of {lmdb_epoch_num}\" f\" epochs with LMDB storage Dataset: {lmdb_total_time:.4f}\")\n",
    "\n",
    "disk_usage_after = shutil.disk_usage(LMDB_cache)\n",
    "print(f\"----------Disk Status after LMDB cache:\")\n",
    "print(f\"Total: {disk_usage_after[0] // (2**30)} GB\")\n",
    "print(f\"Used: {disk_usage_after[1] // (2**30)} GB\")\n",
    "print(f\"Free: {disk_usage_after[2] // (2**30)} GB\")\n",
    "\n",
    "save_results_to_csv(\n",
    "    \"nbs/result/dataset_performance/lmdb_results.csv\",\n",
    "    lmdb_epoch_loss_values, \n",
    "    lmdb_metric_values, \n",
    "    lmdb_epoch_times, \n",
    "    lmdb_total_time, \n",
    "    lmdb_epoch_num,\n",
    "    init_time=lmdb_init_time,\n",
    "    disk_usage_before=disk_usage_before,\n",
    "    disk_usage_after=disk_usage_after\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable deterministic training and `CacheDataset`\n",
    "\n",
    "Precompute all non-random transforms of original data and store in memory.\n",
    "\n",
    "When `runtime_cache=\"processes\"` the cache initialization time `cache_init_time` is negligible.\n",
    "Set `runtime_cache=False` to enable precomputing cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n",
    "log_dir = f\"result/dataset_performance/CacheDataset\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "train_trans, val_trans = transformations()\n",
    "cache_init_start = time.time()\n",
    "cache_train_ds = CacheDataset(\n",
    "    data=train_files, transform=train_trans, cache_rate=0.5, num_workers=8, copy_cache=False\n",
    ")\n",
    "cache_val_ds = CacheDataset(\n",
    "    data=val_files, transform=val_trans, cache_rate=0.5, num_workers=8, copy_cache=False\n",
    ")\n",
    "cache_test_ds = CacheDataset(\n",
    "    data=test_files, transform=val_trans, cache_rate=0.5, num_workers=8, copy_cache=False\n",
    ")\n",
    "cache_init_time = time.time() - cache_init_start\n",
    "\n",
    "(\n",
    "    cache_epoch_num,\n",
    "    cache_total_time,\n",
    "    cache_epoch_loss_values,\n",
    "    cache_metric_values,\n",
    "    cache_epoch_times,\n",
    ") = train_process(cache_train_ds, cache_val_ds, cache_test_ds, log_dir)\n",
    "print(f\"total training time of {cache_epoch_num}\" f\" epochs with CacheDataset: {cache_total_time:.4f}\")\n",
    "\n",
    "save_results_to_csv(\n",
    "    \"nbs/result/dataset_performance/cache_results.csv\",\n",
    "    cache_epoch_loss_values, \n",
    "    cache_metric_values, \n",
    "    cache_epoch_times, \n",
    "    cache_total_time, \n",
    "    cache_epoch_num,\n",
    "    init_time=cache_init_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training loss and validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results_from_csv(filename):\n",
    "    epoch_loss_values = []\n",
    "    metric_values = []\n",
    "    epoch_times = []\n",
    "    total_time = None\n",
    "    max_epochs = None\n",
    "    init_time = None\n",
    "    disk_usage_before = None\n",
    "    disk_usage_after = None\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if row[0] == \"max_epochs\":\n",
    "                max_epochs = int(row[1])\n",
    "            elif row[0] == \"total_time\":\n",
    "                total_time = float(row[1])\n",
    "            elif row[0] == \"init_time\":\n",
    "                init_time = float(row[1])\n",
    "            elif row[0].startswith(\"disk_usage_before\"):\n",
    "                if disk_usage_before is None:\n",
    "                    disk_usage_before = {}\n",
    "                key = row[0].replace(\"disk_usage_before_\", \"\").replace(\"_GB\", \"\")\n",
    "                disk_usage_before[key] = int(row[1])\n",
    "            elif row[0].startswith(\"disk_usage_after\"):\n",
    "                if disk_usage_after is None:\n",
    "                    disk_usage_after = {}\n",
    "                key = row[0].replace(\"disk_usage_after_\", \"\").replace(\"_GB\", \"\")\n",
    "                disk_usage_after[key] = int(row[1])\n",
    "            elif row[0] == \"epoch\":\n",
    "                continue\n",
    "            else:\n",
    "                epoch_loss_values.append(float(row[1]) if row[1] else None)\n",
    "                metric_values.append(float(row[2]) if row[2] else None)\n",
    "                epoch_times.append(float(row[3]) if row[3] else None)\n",
    "    \n",
    "    result = {\n",
    "        \"max_epochs\": max_epochs,\n",
    "        \"total_time\": total_time,\n",
    "        \"epoch_loss_values\": epoch_loss_values,\n",
    "        \"metric_values\": metric_values,\n",
    "        \"epoch_times\": epoch_times\n",
    "    }\n",
    "    \n",
    "    if init_time is not None:\n",
    "        result[\"init_time\"] = init_time\n",
    "    if disk_usage_before is not None:\n",
    "        result[\"disk_usage_before\"] = disk_usage_before\n",
    "    if disk_usage_after is not None:\n",
    "        result[\"disk_usage_after\"] = disk_usage_after\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular dataset\n",
    "regular_results = load_results_from_csv(\"nbs/result/dataset_performance/regular_results.csv\")\n",
    "max_epochs = regular_results[\"max_epochs\"]\n",
    "total_time = regular_results[\"total_time\"]\n",
    "epoch_loss_values = regular_results[\"epoch_loss_values\"]\n",
    "metric_values = regular_results[\"metric_values\"]\n",
    "epoch_times = regular_results[\"epoch_times\"]\n",
    "\n",
    "# Persistent dataset\n",
    "persistent_results = load_results_from_csv(\"nbs/result/dataset_performance/persistent_results.csv\")\n",
    "persistence_total_time = persistent_results[\"total_time\"]\n",
    "persistence_epoch_loss_values = persistent_results[\"epoch_loss_values\"]\n",
    "persistence_metric_values = persistent_results[\"metric_values\"]\n",
    "persistence_epoch_times = persistent_results[\"epoch_times\"]\n",
    "persistence_init_time = persistent_results.get(\"init_time\")\n",
    "persistence_disk_before = persistent_results.get(\"disk_usage_before\")\n",
    "persistence_disk_after = persistent_results.get(\"disk_usage_after\")\n",
    "\n",
    "# LMDB dataset\n",
    "lmdb_results = load_results_from_csv(\"nbs/result/dataset_performance/lmdb_results.csv\")\n",
    "lmdb_total_time = lmdb_results[\"total_time\"]\n",
    "lmdb_epoch_loss_values = lmdb_results[\"epoch_loss_values\"]\n",
    "lmdb_metric_values = lmdb_results[\"metric_values\"]\n",
    "lmdb_epoch_times = lmdb_results[\"epoch_times\"]\n",
    "lmdb_init_time = lmdb_results.get(\"init_time\")\n",
    "lmdb_disk_before = lmdb_results.get(\"disk_usage_before\")\n",
    "lmdb_disk_after = lmdb_results.get(\"disk_usage_after\")\n",
    "\n",
    "# Cache dataset\n",
    "cache_results = load_results_from_csv(\"nbs/result/dataset_performance/cache_results.csv\")\n",
    "cache_total_time = cache_results[\"total_time\"]\n",
    "cache_epoch_loss_values = cache_results[\"epoch_loss_values\"]\n",
    "cache_metric_values = cache_results[\"metric_values\"]\n",
    "cache_epoch_times = cache_results[\"epoch_times\"]\n",
    "cache_init_time = cache_results.get(\"init_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 18))\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "plt.title(\"Regular Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.grid(alpha=0.4, linestyle=\":\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "plt.title(\"Regular Val Mean Dice\")\n",
    "x = [i + 1 for i in range(len(metric_values))]\n",
    "y = cache_metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.grid(alpha=0.4, linestyle=\":\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "plt.title(\"PersistentDataset Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(persistence_epoch_loss_values))]\n",
    "y = persistence_epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.grid(alpha=0.4, linestyle=\":\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "plt.title(\"PersistentDataset Val Mean Dice\")\n",
    "x = [i + 1 for i in range(len(persistence_metric_values))]\n",
    "y = persistence_metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.grid(alpha=0.4, linestyle=\":\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "\n",
    "plt.subplot(4, 2, 5)\n",
    "plt.title(\"LMDBDataset Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(lmdb_epoch_loss_values))]\n",
    "y = lmdb_epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.grid(alpha=0.4, linestyle=\":\")\n",
    "plt.plot(x, y, color=\"yellow\")\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "plt.title(\"LMDBDataset Val Mean Dice\")\n",
    "x = [i + 1 for i in range(len(lmdb_metric_values))]\n",
    "y = lmdb_metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.grid(alpha=0.4, linestyle=\":\")\n",
    "plt.plot(x, y, color=\"yellow\")\n",
    "\n",
    "plt.subplot(4, 2, 7)\n",
    "plt.title(\"Cache Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(cache_epoch_loss_values))]\n",
    "y = cache_epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.grid(alpha=0.4, linestyle=\":\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "\n",
    "plt.subplot(4, 2, 8)\n",
    "plt.title(\"Cache Val Mean Dice\")\n",
    "x = [i + 1 for i in range(len(cache_metric_values))]\n",
    "y = cache_metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.grid(alpha=0.4, linestyle=\":\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot total time and every epoch time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(f\"Total Train Time ({max_epochs} epochs)\")\n",
    "plt.bar(\"regular\", total_time, 1, label=\"Regular Dataset\", color=\"red\")\n",
    "plt.bar(\n",
    "    \"lmdb\",\n",
    "    lmdb_init_time + lmdb_total_time,\n",
    "    1,\n",
    "    label=\"LMDB cache init\",\n",
    "    color=\"yellow\",\n",
    ")\n",
    "plt.bar(\"lmdb\", lmdb_total_time, 1, label=\"LMDB Dataset\", color=\"orange\")\n",
    "plt.bar(\n",
    "    \"persistent\",\n",
    "    persistence_init_time + persistence_total_time,\n",
    "    1,\n",
    "    label=\"Persistent Dataset\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "if persistence_init_time > 1:\n",
    "    plt.bar(\"persistent\", persistence_init_time, 1, label=\"Persistent Init\", color=\"pink\")\n",
    "plt.bar(\n",
    "    \"cache\",\n",
    "    cache_init_time + cache_total_time,\n",
    "    1,\n",
    "    label=\"Cache Dataset\",\n",
    "    color=\"green\",\n",
    ")\n",
    "if cache_init_time > 1:\n",
    "    plt.bar(\"cache\", cache_init_time, 1, label=\"Cache Init\", color=\"grey\")\n",
    "plt.ylabel(\"secs\")\n",
    "plt.grid(alpha=0.4, linestyle=\":\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Epoch Time\")\n",
    "x = [i + 1 for i in range(len(epoch_times))]\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"secs\")\n",
    "plt.plot(x, epoch_times, label=\"Regular Dataset\", color=\"red\")\n",
    "plt.plot(x, persistence_epoch_times, label=\"Persistent Dataset\", color=\"blue\")\n",
    "plt.plot(x, lmdb_epoch_times, label=\"LMDB Dataset\", color=\"yellow\")\n",
    "plt.plot(x, cache_epoch_times, label=\"Cache Dataset\", color=\"green\")\n",
    "plt.grid(alpha=0.4, linestyle=\":\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Disk Usage `Persistent` and `LMDBDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 그래프 스타일 설정\n",
    "plt.style.use('default') \n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# 데이터 준비\n",
    "datasets = ['Persistent Dataset', 'LMDB Dataset']\n",
    "disk_usage_changes = [\n",
    "    persistence_disk_after['used'] - persistence_disk_before['used'] if (persistence_disk_after and persistence_disk_before) else 0,\n",
    "    lmdb_disk_after['used'] - lmdb_disk_before['used'] if (lmdb_disk_after and lmdb_disk_before) else 0\n",
    "]\n",
    "\n",
    "# 막대 그래프 생성\n",
    "bars = plt.bar(datasets, disk_usage_changes, color=['skyblue', 'lightcoral'], width=0.5)\n",
    "\n",
    "# 그래프 꾸미기\n",
    "plt.xlabel('Dataset Type', fontsize=12)\n",
    "plt.ylabel('Disk Usage Increase (GB)', fontsize=12)\n",
    "plt.title('Comparison of Disk Usage Increase by Dataset Type', fontsize=14)\n",
    "\n",
    "# 막대 위에 값 표시\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height,\n",
    "             f'{height:.1f}GB',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 수치 비교 출력\n",
    "print(\"\\nDisk Usage Increase Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "for dataset, change in zip(datasets, disk_usage_changes):\n",
    "    print(f\"{dataset}: {change:.1f} GB\")\n",
    "\n",
    "# 어떤 데이터셋이 더 효율적인지 비교\n",
    "if disk_usage_changes[0] != disk_usage_changes[1]:\n",
    "    more_efficient = datasets[0] if disk_usage_changes[0] < disk_usage_changes[1] else datasets[1]\n",
    "    difference = abs(disk_usage_changes[0] - disk_usage_changes[1])\n",
    "    print(f\"\\n{more_efficient}가 디스크 사용량이 {difference:.1f}GB 더 적습니다.\")\n",
    "else:\n",
    "    print(\"\\n두 데이터셋의 디스크 사용량이 동일합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
